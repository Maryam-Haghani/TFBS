device: "cuda"
eval_batch_size: 32

model:
  model_name: "HyenaDNA"
  model_version: "hyenadna-tiny-1k-seqlen"
  use_padding: true
  max_length: 264
  finetune_type: "standard" # options: LoRA, standard
  pooling_method: "mean"
  use_pretrained_model: true
  saved_model_name: "freeze_layers_none_train_batch_size_32_learning_rate_0.0001_weight_decay_0.1"

# required when use_saved_model is true
output_dir: "/projects/intro2gds/Maryam_HyenaDNA_202503/HyenaDNA/TFBS/outputs"