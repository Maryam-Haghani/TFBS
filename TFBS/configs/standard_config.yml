# standard_config.yml

paths:
  dataset_path: "../inputs/AtABFs_training_shuffle_neg_stride_200.csv"
  dataset_split_path: "../inputs/data_splits/AtABFstraining_shuffle_neg_stride_200"
  model_dir: "../models/AtABFs/standard"
  test_result_dir: "../outputs/AtABFs/standard/test_results"
  loss_dir: "../outputs/AtABFs/standard/loss"
  
dataset_split:
  split_type: "random"       # Options: 'random' or 'cross_id'
  id_column: "chromosomeId"
  train_ids: [1, 3, 5]
  val_ids: [2]
  test_ids: [4]
  train_size: 0.7
  random_state: 436346
  
model:
  use_padding: true
  model_max_length: 350
  pretrained_model_name: "hyenadna-tiny-1k-seqlen"
      # 'hyenadna-tiny-1k-seqlen'
      # 'hyenadna-tiny-1k-seqlen-d256'
      # 'hyenadna-tiny-16k-seqlen-d128''
      # 'hyenadna-small-32k-seqlen'
      # 'hyenadna-medium-160k-seqlen'
      # 'hyenadna-medium-450k-seqlen'
      # 'hyenadna-large-1m-seqlen'
  is_finetuned: true
  use_saved_model: false
  saved_finetuned_model_name: "fine-tuned_model_batch_size_32_learning_rate_1e-4_weight_decay_0.1_freeze_layers-.pt"
  # "fine-tuned_model_batch_size_32_learning_rate_1e-4_weight_decay_0.1_freeze_layers-.pt"

device: "cuda"
  
training:
  num_epochs: 60
  model_params:
    batch_size: [32]
    learning_rate: [1e-4]
    weight_decay: [0.1]
#    batch_size: [ 32, 64, 128, 256 ]
#    learning_rate: [ 1e-6, 1e-5, 1e-4, 1e-3 ]
#    weight_decay: [ 0.0, 0.01, 0.1 ]
  early_stopping:
    patience: 5
    delta: 0.0
  freeze_layers: ['backbone', 'backbone.embeddings', 'backbone.layers', 'none']
  # 'backbone.embeddings': Freezes the embedding layer.
  # 'backbone.layers'    : Freezes all transformer blocks.
  # 'backbone.layers.[i]': Freezes a specific transformer block.
  # 'backbone'           : Freezes the entire backbone (embeddings + all transformer layers).
  # 'head'               : Freezes the output classification head.#  'fine_tuned_hyena_hyenadna-tiny-1k-seqlen_random_0.7_0.15_0.15_epoch_6.pt'