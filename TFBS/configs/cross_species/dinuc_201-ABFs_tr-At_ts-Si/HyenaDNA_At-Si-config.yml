dataset_path: ["/projects/intro2gds/Maryam_HyenaDNA_202503/HyenaDNA/TFBS/inputs/AtABFs_dinuc_shuffle_neg_fixed_201.csv",
               "/projects/intro2gds/Maryam_HyenaDNA_202503/HyenaDNA/TFBS/inputs/SiABFs_dinuc_shuffle_neg_fixed_201.csv"]
output_dir: "/projects/intro2gds/Maryam_HyenaDNA_202503/HyenaDNA/TFBS/outputs"

name: "ABFs_dinuc_201_cross-species_tr-At_test-Si"
  
dataset_split:
  dir: "/projects/intro2gds/Maryam_HyenaDNA_202503/HyenaDNA/TFBS/inputs/data_splits"
  test_split_type: "cross"  # Options: 'random' or 'cross'
  val_split_type: "n-fold" # 'cross', 'n-fold': can be 'cross' when test_split_type is 'cross', as well
  id_column: "species"
  val_ids: [] # should be assigned for 'cross' val_split_type
  test_ids: ["Si"] # should be assigned for 'cross' test_split_type
  test_size: 0.2 # should be assigned for 'random' test_split_type
  fold: 5 # should be > 1 for 'n-fold' val_split_type, else 1
  random_state: 436346
  partition_mode: 'relaxed' # Options: 'relaxed' or 'strict'
  sim: 0.9 # required when 'partition_mode' is strict
  word_size: 8 # required when 'partition_mode' is strict

model:
  model_name: "HyenaDNA"
  model_version: "hyenadna-tiny-1k-seqlen"
  use_padding: true
  max_length: 201
  use_saved_model: False
#  saved_model_parent_dir: "/projects/intro2gds/Maryam_HyenaDNA_202503/HyenaDNA/TFBS/outputs/ABFs_cross-species_tr-At_test-Si/HyenaDNA/hyenadna-tiny-1k-seqlen/standard/relaxed/models/"
#  saved_model_name: ["fold-1_train_batch_size_32_learning_rate_1e-4_weight_decay_0.1_freeze_layers_['none']_eval_batch_size_32.pt",
#                    "fold-2_train_batch_size_32_learning_rate_1e-4_weight_decay_0.1_freeze_layers_['none']_eval_batch_size_32.pt",
#                    "fold-3_train_batch_size_32_learning_rate_1e-4_weight_decay_0.1_freeze_layers_['none']_eval_batch_size_32.pt",
#                    "fold-4_train_batch_size_32_learning_rate_1e-4_weight_decay_0.1_freeze_layers_['none']_eval_batch_size_32.pt",
#                    "fold-5_train_batch_size_32_learning_rate_1e-4_weight_decay_0.1_freeze_layers_['none']_eval_batch_size_32.pt"]
  finetune_type: "standard" # options: LoRA, standard

device: "cuda"
  
training:
  num_epochs: 60
  adjustable_LR:
    enabled: false
  model_params:
    train_batch_size: [32]
    learning_rate: [1e-4]
    weight_decay: [0.1]
    freeze_layers: [['backbone'], ['backbone.embeddings'], ['backbone.layers'], ['none']]
  early_stopping:
    patience: 10
    delta: 0.0

wandb:
  enabled: True
  token: 'dc3d431c0afb735d9ac046c72f076bf1f7656472'
  entity_name: 'haghani-vt'
  timezone: 'US/Eastern'
  timezone_format: '%Y-%m-%d_%H-%M-%S'